import pandas as pd
import pandas_ta as ta
import logging
from datetime import datetime, timedelta
from typing import Union, Tuple, Dict, List

# é…ç½®æ—¥å¿— (ç§»åŠ¨åˆ°æœ€å‰é¢)
# å°†æ—¥å¿—çº§åˆ«æ”¹ä¸º DEBUG ä»¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - [%(module)s:%(funcName)s:%(lineno)d] - %(message)s')
logger = logging.getLogger(__name__)

# å‡è®¾æ•°æ®è·å–æ¨¡å—çš„æ–‡ä»¶åä¸º æ•°æ®è·å–æ¨¡å—.py
try:
    import æ•°æ®è·å–æ¨¡å—
except ImportError:
    # ç°åœ¨å¯ä»¥ä½¿ç”¨ logger äº†
    logger.error("æ— æ³•å¯¼å…¥ 'æ•°æ®è·å–æ¨¡å—.py'ï¼Œè¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨ä¸”è·¯å¾„æ­£ç¡®ã€‚")
    # å¯ä»¥é€‰æ‹©æŠ›å‡ºå¼‚å¸¸æˆ–è®¾ç½®ä¸€ä¸ªæ ‡å¿—ä½
    æ•°æ®è·å–æ¨¡å— = None 

# å¯¼å…¥é…ç½®æ¨¡å—
try:
    import é…ç½®
    # å°è¯•å¯¼å…¥é…ç½®å­—å…¸ï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›æ›´å…·ä½“çš„é”™è¯¯
    try:
        from é…ç½® import MICRO_TREND_CONFIG
    except ImportError:
        logger.error("æˆåŠŸå¯¼å…¥ é…ç½®.py ä½†æœªæ‰¾åˆ° MICRO_TREND_CONFIG å˜é‡ã€‚è¯·æ£€æŸ¥è¯¥æ–‡ä»¶ã€‚")
        MICRO_TREND_CONFIG = {} # è®¾ç½®é»˜è®¤ç©ºå­—å…¸
        if é…ç½® is None: # å¦‚æœé…ç½®æ¨¡å—ä¹ŸæœªåŠ è½½ (è™½ç„¶ç†è®ºä¸Šä¸åº”å‘ç”Ÿåœ¨æ­¤å¤„)
            é…ç½® = type('obj', (object,), {})() # åˆ›å»ºä¸€ä¸ªç©ºå¯¹è±¡ä»¥é˜² AttributeError
           
except SyntaxError as e:
    logger.error(f"å¯¼å…¥ é…ç½®.py æ—¶å‘ç”Ÿè¯­æ³•é”™è¯¯: {e}", exc_info=True)
    é…ç½® = None
    MICRO_TREND_CONFIG = {}
except ImportError as e:
    logger.error(f"å¯¼å…¥ é…ç½®.py æ—¶å‘ç”Ÿ ImportError (å¯èƒ½æ–‡ä»¶ä¸å­˜åœ¨æˆ–è·¯å¾„é—®é¢˜): {e}", exc_info=True)
    é…ç½® = None
    MICRO_TREND_CONFIG = {}
except Exception as e: # æ•è·å…¶ä»–å¯èƒ½çš„å¯¼å…¥é”™è¯¯
    logger.error(f"å¯¼å…¥ é…ç½®.py æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
    é…ç½® = None
    MICRO_TREND_CONFIG = {}

# æ—¥å¿—é…ç½®ä»£ç å—å·²ç§»åŠ¨åˆ°å‰é¢
# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - [%(module)s:%(funcName)s:%(lineno)d] - %(message)s')
# logger = logging.getLogger(__name__)

# --- å†…éƒ¨è¾…åŠ©å‡½æ•°ï¼šè§£è¯»æŒ‡æ ‡ ---
def _interpret_indicators(last_row: pd.Series, prev_row: pd.Series, config: dict,
                        ema_short_col: str, ema_long_col: str, roc_col: str,
                        rsi_col: str, bb_upper_col: str, bb_lower_col: str, bb_mid_col: str,
                        macd_hist_col: str, volume_col: str, volume_ma_col: str,
                        kdj_k_col: str, kdj_d_col: str, kdj_j_col: str,
                        ichi_tenkan_col: str, ichi_kijun_col: str, ichi_senkou_a_col: str, ichi_senkou_b_col: str, ichi_chikou_col: str,
                        adx_col: str, adx_dip_col: str, adx_din_col: str
                        ) -> Dict[str, Union[str, float, Dict[str, str]]]:
    """
    è§£è¯»å•ä¸ªæ—¶é—´ç‚¹çš„æ‰€æœ‰æŒ‡æ ‡ï¼Œç”Ÿæˆè¯¦ç»†è§£è¯»ã€è¯„åˆ†å’Œç»„åˆä¿¡å·ã€‚

    Args:
        last_row (pd.Series): æœ€æ–°çš„åŒ…å«æŒ‡æ ‡çš„ K çº¿æ•°æ®è¡Œã€‚
        prev_row (pd.Series): å€’æ•°ç¬¬äºŒè¡Œ K çº¿æ•°æ®ï¼Œç”¨äºè®¡ç®—äº¤å‰ç­‰ã€‚
        config (dict): åŒ…å«è§£è¯»é˜ˆå€¼çš„é…ç½® (e.g., rsi_oversold, adx_threshold)ã€‚
        ... (column names for all indicators) ...

    Returns:
        Dict: åŒ…å« 'combined_signal', 'score', 'details' çš„å­—å…¸ã€‚
              'details' æ˜¯ä¸€ä¸ªåŒ…å«å„é¡¹æŒ‡æ ‡è§£è¯»å­—ç¬¦ä¸²çš„å­å­—å…¸ã€‚
    """
    interpretations = {}
    combined_signal_str = "â†”ï¸" # Default signal string
    score = 0.0 # Default score
    params = config.get('PARAMS', {})
    thresholds_interpret = config.get('THRESHOLDS', {}) # Thresholds for interpretation
    # !!! Get Scoring Config !!!
    scoring_config = config.get('SCORING', {})
    weights = scoring_config.get('WEIGHTS', {}) # Scoring weights
    thresholds_score = scoring_config.get('THRESHOLDS', {}) # Scoring thresholds

    try:
        # --- æå–åŸºç¡€æ•°æ® ---
        close = last_row['close']
        # --- ç°æœ‰æŒ‡æ ‡è§£è¯» --- 
        # 1. EMA Trend
        ema_short = last_row.get(ema_short_col, pd.NA)
        ema_long = last_row.get(ema_long_col, pd.NA)
        trend_strength_threshold = thresholds_interpret.get('trend_strength_threshold', 0.001) # Example threshold
        ema_trend = "è¶‹åŠ¿: æœªçŸ¥"
        if pd.notna(ema_short) and pd.notna(ema_long):
            diff_pct = (ema_short - ema_long) / ema_long if ema_long else 0
            if diff_pct > trend_strength_threshold * 2: ema_trend = "è¶‹åŠ¿: å¼ºå‡(â†‘â†‘)"
            elif diff_pct > 0: ema_trend = "è¶‹åŠ¿: ä¸Šå‡(â†‘)"
            elif diff_pct < -trend_strength_threshold * 2: ema_trend = "è¶‹åŠ¿: å¼ºé™(â†“â†“)"
            elif diff_pct < 0: ema_trend = "è¶‹åŠ¿: ä¸‹é™(â†“)"
            else: ema_trend = "è¶‹åŠ¿: ç›˜æ•´(â†”)"
        interpretations['ema_trend'] = ema_trend

        # 2. ROC Momentum
        roc = last_row.get(roc_col, pd.NA)
        mom_strength_threshold = thresholds_interpret.get('momentum_strength_threshold', 0.1) # Example threshold
        roc_momentum = "åŠ¨é‡: æœªçŸ¥"
        if pd.notna(roc):
            if roc > mom_strength_threshold * 2: roc_momentum = "åŠ¨é‡: å¼ºæ­£(++)"
            elif roc > 0: roc_momentum = "åŠ¨é‡: æ­£å‘(+)"
            elif roc < -mom_strength_threshold * 2: roc_momentum = "åŠ¨é‡: å¼ºè´Ÿ(--)"
            elif roc < 0: roc_momentum = "åŠ¨é‡: è´Ÿå‘(-)"
            else: roc_momentum = "åŠ¨é‡: è¶‹å¹³(0)"
        interpretations['roc_momentum'] = roc_momentum

        # 3. RSI Overbought/Oversold
        rsi = last_row.get(rsi_col, pd.NA)
        oversold = thresholds_interpret.get('rsi_oversold', 30)
        overbought = thresholds_interpret.get('rsi_overbought', 70)
        rsi_state = "RSI: æœªçŸ¥"
        if pd.notna(rsi):
            if rsi > overbought: rsi_state = f"RSI: è¶…ä¹°(>{overbought})"
            elif rsi < oversold: rsi_state = f"RSI: è¶…å–(<{oversold})"
            else: rsi_state = "RSI: ä¸­æ€§"
        interpretations['rsi_state'] = rsi_state

        # 4. Bollinger Bands Position
        bb_upper = last_row.get(bb_upper_col, pd.NA)
        bb_lower = last_row.get(bb_lower_col, pd.NA)
        bb_mid = last_row.get(bb_mid_col, pd.NA) # Get Mid Band value
        bb_pos = "BB: æœªçŸ¥"
        if pd.notna(close) and pd.notna(bb_upper) and pd.notna(bb_lower) and pd.notna(bb_mid):
            if close > bb_upper: bb_pos = "BB: ç©¿ä¸Šè½¨(â†—â†—)" 
            elif close < bb_lower: bb_pos = "BB: ç©¿ä¸‹è½¨(â†˜â†˜)" 
            # Use a small tolerance for touching
            elif abs(close - bb_upper) < (bb_upper - bb_mid) * 0.05 : bb_pos = "BB: è§¦ä¸Šè½¨(â†—)" 
            elif abs(close - bb_lower) < (bb_mid - bb_lower) * 0.05 : bb_pos = "BB: è§¦ä¸‹è½¨(â†˜)"
            elif close > bb_mid: bb_pos = "BB: ä¸­è½¨ä¸Šæ–¹"
            elif close < bb_mid: bb_pos = "BB: ä¸­è½¨ä¸‹æ–¹"
            else: bb_pos = "BB: åœ¨ä¸­è½¨"
        interpretations['bb_pos'] = bb_pos

        # 5. MACD Histogram and Cross
        macd_hist = last_row.get(macd_hist_col, pd.NA)
        prev_macd_hist = prev_row.get(macd_hist_col, pd.NA) if prev_row is not None else pd.NA
        macd_cross = ""
        macd_hist_state = ""
        if pd.notna(macd_hist):
            if macd_hist > 0:
                 macd_hist_state = "æŸ±>0"
                 if pd.notna(prev_macd_hist) and prev_macd_hist <= 0: macd_cross = ",é‡‘å‰(ğŸ”¼)" # Gold Cross
            elif macd_hist < 0:
                 macd_hist_state = "æŸ±<0"
                 if pd.notna(prev_macd_hist) and prev_macd_hist >= 0: macd_cross = ",æ­»å‰(ğŸ”½)" # Dead Cross
            else:
                 macd_hist_state = "æŸ±=0"
        macd_combined_state = f"MACD:{macd_hist_state}{macd_cross}"
        interpretations['macd_state'] = macd_combined_state
        interpretations['macd_cross'] = macd_cross # Store cross separately if needed for logic
        interpretations['macd_hist_state'] = macd_hist_state # Store state separately

        # 6. Volume Analysis
        volume = last_row.get(volume_col, pd.NA)
        volume_ma = last_row.get(volume_ma_col, pd.NA)
        volume_increase_threshold = thresholds_interpret.get('volume_increase_threshold', 1.2)
        volume_state = "Vol: æœªçŸ¥"
        if pd.notna(volume) and pd.notna(volume_ma) and volume_ma > 0:
            if volume > volume_ma * volume_increase_threshold: volume_state = "Vol: æ”¾é‡(ğŸ“ˆ)"
            elif volume < volume_ma / volume_increase_threshold: volume_state = "Vol: ç¼©é‡(ğŸ“‰)"
            else: volume_state = "Vol: å¹³é‡"
        interpretations['volume_state'] = volume_state

        # --- æ–°æŒ‡æ ‡è§£è¯» --- 
        
        # 7. KDJ Interpretation
        kdj_k = last_row.get(kdj_k_col, pd.NA)
        kdj_d = last_row.get(kdj_d_col, pd.NA)
        kdj_j = last_row.get(kdj_j_col, pd.NA)
        prev_k = prev_row.get(kdj_k_col, pd.NA) if prev_row is not None else pd.NA
        prev_d = prev_row.get(kdj_d_col, pd.NA) if prev_row is not None else pd.NA
        kdj_cross = ""
        kdj_level = ""
        kdj_signal_str = "KDJ: æœªçŸ¥"
        if pd.notna(kdj_k) and pd.notna(kdj_d) and pd.notna(kdj_j) and pd.notna(prev_k) and pd.notna(prev_d):
            # KDJ Cross
            if kdj_k > kdj_d and prev_k <= prev_d: kdj_cross = ",é‡‘å‰(ğŸ”¼)" 
            elif kdj_k < kdj_d and prev_k >= prev_d: kdj_cross = ",æ­»å‰(ğŸ”½)"
            # J Level (common thresholds: 80-100 overbought, 0-20 oversold)
            kdj_overbought = thresholds_interpret.get('kdj_overbought', 80)
            kdj_oversold = thresholds_interpret.get('kdj_oversold', 20)
            if kdj_j > kdj_overbought: kdj_level = f",Jè¶…ä¹°(>{kdj_overbought})"
            elif kdj_j < kdj_oversold: kdj_level = f",Jè¶…å–(<{kdj_oversold})"
            kdj_signal_str = f"KDJ: K>D" if kdj_k > kdj_d else "KDJ: K<D" 
            kdj_signal_str += kdj_cross + kdj_level
        elif pd.notna(kdj_k) and pd.notna(kdj_d): # Handle case with no prev data
             kdj_signal_str = f"KDJ: K>D" if kdj_k > kdj_d else "KDJ: K<D" 
             # J Level interpretation still possible
             kdj_overbought = thresholds_interpret.get('kdj_overbought', 80)
             kdj_oversold = thresholds_interpret.get('kdj_oversold', 20)
             if pd.notna(kdj_j):
                 if kdj_j > kdj_overbought: kdj_level = f",Jè¶…ä¹°(>{kdj_overbought})"
                 elif kdj_j < kdj_oversold: kdj_level = f",Jè¶…å–(<{kdj_oversold})"
             kdj_signal_str += kdj_level
        interpretations['kdj_signal'] = kdj_signal_str

        # 8. Ichimoku Interpretation
        tenkan = last_row.get(ichi_tenkan_col, pd.NA)
        kijun = last_row.get(ichi_kijun_col, pd.NA)
        senkou_a = last_row.get(ichi_senkou_a_col, pd.NA)
        senkou_b = last_row.get(ichi_senkou_b_col, pd.NA)
        chikou = last_row.get(ichi_chikou_col, pd.NA)
        # Note: Chikou needs comparison against price N periods ago (e.g., 26). This row doesn't have it directly.
        # We can only compare Chikou with the *current* close for a rough idea, or ignore Chikou for simplicity here.
        ichi_signal_parts = []
        if pd.notna(close) and pd.notna(senkou_a) and pd.notna(senkou_b):
            if close > senkou_a and close > senkou_b: ichi_signal_parts.append("ä»·åœ¨äº‘ä¸Š")
            elif close < senkou_a and close < senkou_b: ichi_signal_parts.append("ä»·åœ¨äº‘ä¸‹")
            else: ichi_signal_parts.append("ä»·åœ¨äº‘ä¸­")
        if pd.notna(tenkan) and pd.notna(kijun):
            if tenkan > kijun: ichi_signal_parts.append("è½¬>åŸº") # Tenkan > Kijun (Bullish)
            elif tenkan < kijun: ichi_signal_parts.append("è½¬<åŸº") # Tenkan < Kijun (Bearish)
            else: ichi_signal_parts.append("è½¬=åŸº")
        # Chikou interpretation (simplified: vs current close - less accurate)
        if pd.notna(chikou) and pd.notna(close):
             if chikou > close: ichi_signal_parts.append("è¿Ÿ>ä»·")
             elif chikou < close: ichi_signal_parts.append("è¿Ÿ<ä»·")
        ichi_signal_str = "Ichi: " + (", ".join(ichi_signal_parts) if ichi_signal_parts else "æœªçŸ¥")
        interpretations['ichi_signal'] = ichi_signal_str

        # 9. ADX Interpretation
        adx = last_row.get(adx_col, pd.NA)
        adx_dip = last_row.get(adx_dip_col, pd.NA) # +DI
        adx_din = last_row.get(adx_din_col, pd.NA) # -DI
        adx_trend_threshold = thresholds_interpret.get('adx_trend_threshold', 25) # Common threshold for trending market
        adx_weak_threshold = thresholds_interpret.get('adx_weak_threshold', 20)
        adx_signal_parts = []
        if pd.notna(adx):
            if adx > adx_trend_threshold: adx_signal_parts.append(f"å¼ºè¶‹(>{adx_trend_threshold:.0f})")
            elif adx < adx_weak_threshold: adx_signal_parts.append(f"å¼±è¶‹(<{adx_weak_threshold:.0f})")
            else: adx_signal_parts.append("è¶‹ä¸­")
        if pd.notna(adx_dip) and pd.notna(adx_din):
            if adx_dip > adx_din: adx_signal_parts.append("+DI>-DI") # Uptrend bias
            elif adx_din > adx_dip: adx_signal_parts.append("-DI>+DI") # Downtrend bias
            else: adx_signal_parts.append("+DI=-DI")
        adx_signal_str = "ADX: " + (", ".join(adx_signal_parts) if adx_signal_parts else "æœªçŸ¥")
        interpretations['adx_signal'] = adx_signal_str
        
        # --- æå–çŠ¶æ€å˜é‡ (ç”¨äºè®¡åˆ†) ---
        # (This logic remains the same)
        is_strong_bull_trend = interpretations.get('ema_trend') == "è¶‹åŠ¿: å¼ºå‡(â†‘â†‘)"
        is_bull_trend = interpretations.get('ema_trend') == "è¶‹åŠ¿: ä¸Šå‡(â†‘)"
        is_strong_bear_trend = interpretations.get('ema_trend') == "è¶‹åŠ¿: å¼ºé™(â†“â†“)"
        is_bear_trend = interpretations.get('ema_trend') == "è¶‹åŠ¿: ä¸‹é™(â†“)"
        is_ranging_trend = interpretations.get('ema_trend') == "è¶‹åŠ¿: ç›˜æ•´(â†”)"
        is_strong_pos_mom = interpretations.get('roc_momentum') == "åŠ¨é‡: å¼ºæ­£(++)"
        is_pos_mom = interpretations.get('roc_momentum') == "åŠ¨é‡: æ­£å‘(+)"
        is_strong_neg_mom = interpretations.get('roc_momentum') == "åŠ¨é‡: å¼ºè´Ÿ(--)"
        is_neg_mom = interpretations.get('roc_momentum') == "åŠ¨é‡: è´Ÿå‘(-)"
        is_weak_mom = interpretations.get('roc_momentum') == "åŠ¨é‡: è¶‹å¹³(0)"
        is_overbought = "è¶…ä¹°" in interpretations.get('rsi_state', "")
        is_oversold = "è¶…å–" in interpretations.get('rsi_state', "")
        is_macd_gold_cross = "é‡‘å‰" in interpretations.get('macd_state', "")
        is_macd_dead_cross = "æ­»å‰" in interpretations.get('macd_state', "")
        is_macd_hist_pos = "æŸ±>0" in interpretations.get('macd_state', "")
        is_macd_hist_neg = "æŸ±<0" in interpretations.get('macd_state', "")
        is_volume_high = "æ”¾é‡" in interpretations.get('volume_state', "") # Volume score not implemented yet
        is_kdj_gold_cross = "é‡‘å‰" in interpretations.get('kdj_signal', "")
        is_kdj_dead_cross = "æ­»å‰" in interpretations.get('kdj_signal', "")
        is_kdj_overbought = "Jè¶…ä¹°" in interpretations.get('kdj_signal', "")
        is_kdj_oversold = "Jè¶…å–" in interpretations.get('kdj_signal', "")
        is_kdj_k_above_d = "K>D" in interpretations.get('kdj_signal', "")
        is_kdj_d_above_k = "K<D" in interpretations.get('kdj_signal', "")
        is_price_above_cloud = "ä»·åœ¨äº‘ä¸Š" in interpretations.get('ichi_signal', "")
        is_price_below_cloud = "ä»·åœ¨äº‘ä¸‹" in interpretations.get('ichi_signal', "")
        is_tenkan_above_kijun = "è½¬>åŸº" in interpretations.get('ichi_signal', "")
        is_kijun_above_tenkan = "è½¬<åŸº" in interpretations.get('ichi_signal', "")
        is_adx_strong_trend = "å¼ºè¶‹" in interpretations.get('adx_signal', "")
        is_adx_medium_trend = "è¶‹ä¸­" in interpretations.get('adx_signal', "")
        is_adx_weak_trend = "å¼±è¶‹" in interpretations.get('adx_signal', "")
        is_adx_pos_dominant = "+DI>-DI" in interpretations.get('adx_signal', "")
        is_adx_neg_dominant = "-DI>+DI" in interpretations.get('adx_signal', "")

        # --- è®¡ç®—æ€»åˆ† --- 
        score = 0
        if not weights: 
             logging.warning("è¯„åˆ†æƒé‡æœªåœ¨é…ç½®ä¸­å®šä¹‰ï¼Œæ— æ³•è®¡ç®—åˆ†æ•°ã€‚")
        else:
            # Trend
            if is_strong_bull_trend: score += weights.get('strong_bull_trend', 3)
            if is_bull_trend: score += weights.get('bull_trend', 1)
            if is_strong_bear_trend: score += weights.get('strong_bear_trend', -3)
            if is_bear_trend: score += weights.get('bear_trend', -1)
            # Momentum
            if is_strong_pos_mom: score += weights.get('strong_pos_mom', 2)
            if is_pos_mom: score += weights.get('pos_mom', 1)
            if is_strong_neg_mom: score += weights.get('strong_neg_mom', -2)
            if is_neg_mom: score += weights.get('neg_mom', -1)
            # Oscillators
            if is_overbought: score += weights.get('rsi_overbought', -2)
            if is_oversold: score += weights.get('rsi_oversold', 2)
            if is_kdj_overbought: score += weights.get('kdj_overbought', -1)
            if is_kdj_oversold: score += weights.get('kdj_oversold', 1)
            # Crosses
            if is_macd_gold_cross: score += weights.get('macd_gold_cross', 3)
            if is_macd_dead_cross: score += weights.get('macd_dead_cross', -3)
            if is_kdj_gold_cross: score += weights.get('kdj_gold_cross', 2)
            if is_kdj_dead_cross: score += weights.get('kdj_dead_cross', -2)
            # State/Location
            if is_macd_hist_pos: score += weights.get('macd_hist_pos', 1)
            if is_macd_hist_neg: score += weights.get('macd_hist_neg', -1)
            if is_kdj_k_above_d: score += weights.get('kdj_k_above_d', 0.5)
            if is_kdj_d_above_k: score += weights.get('kdj_d_above_k', -0.5)
            if is_price_above_cloud: score += weights.get('price_above_cloud', 2)
            if is_price_below_cloud: score += weights.get('price_below_cloud', -2)
            if is_tenkan_above_kijun: score += weights.get('tenkan_above_kijun', 1)
            if is_kijun_above_tenkan: score += weights.get('kijun_above_tenkan', -1)
            # ADX Confirmation
            if is_adx_strong_trend and is_adx_pos_dominant: score += weights.get('adx_strong_pos', 2)
            if is_adx_medium_trend and is_adx_pos_dominant: score += weights.get('adx_medium_pos', 1)
            if is_adx_strong_trend and is_adx_neg_dominant: score += weights.get('adx_strong_neg', -2)
            if is_adx_medium_trend and is_adx_neg_dominant: score += weights.get('adx_medium_neg', -1)
        
        logging.debug(f"Calculated score: {score}")

        # --- æ˜ å°„åˆ†æ•°åˆ°ä¿¡å· --- 
        strong_bull_thresh = thresholds_score.get('strong_bullish', 8)
        bull_thresh = thresholds_score.get('bullish', 3)
        strong_bear_thresh = thresholds_score.get('strong_bearish', -8)
        bear_thresh = thresholds_score.get('bearish', -3)
        adx_override = thresholds_score.get('adx_weak_override_enabled', True)
        adx_max_signal = thresholds_score.get('adx_weak_max_signal', 1) # 0: force â†”ï¸, 1: cap at +/-
        
        signal_level = 0 # 0: Neutral, 1: Bearish, 2: Strong Bearish, 3: Bullish, 4: Strong Bullish
        
        if score >= strong_bull_thresh: signal_level = 4
        elif score >= bull_thresh: signal_level = 3
        elif score <= strong_bear_thresh: signal_level = 2
        elif score <= bear_thresh: signal_level = 1
        else: signal_level = 0
            
        # Apply ADX Weak Override
        if adx_override and is_adx_weak_trend:
            logging.debug(f"ADX weak trend detected. Applying override (max_signal={adx_max_signal}). Original signal level: {signal_level}")
            if adx_max_signal == 0:
                signal_level = 0 # Force neutral
            elif adx_max_signal == 1:
                # Cap at +/- ; Strong signals become normal, neutral remains neutral
                if signal_level == 4: signal_level = 3 # Strong Bull -> Bull
                elif signal_level == 2: signal_level = 1 # Strong Bear -> Bear
            # If adx_max_signal is > 1, no capping applied based on this setting
            logging.debug(f"Signal level after ADX override: {signal_level}")
            
        # Map final level to signal string
        if signal_level == 4: combined_signal_str = "å¼ºçœ‹æ¶¨(ğŸš€)"
        elif signal_level == 3: combined_signal_str = "çœ‹æ¶¨(+)"
        elif signal_level == 2: combined_signal_str = "å¼ºçœ‹è·Œ(ğŸ’¥)"
        elif signal_level == 1: combined_signal_str = "çœ‹è·Œ(-)"
        else: combined_signal_str = "ç›˜æ•´/ä¸æ˜(â†”ï¸)"
            
        # --- è¿”å›ç»“æ„åŒ–ç»“æœ --- 
        return {
            "combined_signal": combined_signal_str,
            "score": round(score, 2), # Round score for cleaner output
            "details": interpretations # Return the dictionary with detailed interpretations
        }

    except Exception as e:
        logging.error(f"Error interpreting indicators: {e}", exc_info=True)
        # Return an error structure
        return {
            "combined_signal": "é”™è¯¯",
            "score": 0.0,
            "details": {"error": "æ— æ³•è§£è¯»æŒ‡æ ‡"}
        }

# --- æŒ‡æ ‡è®¡ç®— ---
def _calculate_indicators(df: pd.DataFrame, config: dict) -> pd.DataFrame:
    """
    è®¡ç®—æ‰€æœ‰éœ€è¦çš„æŠ€æœ¯æŒ‡æ ‡ã€‚
    """
    params = config.get('PARAMS', {}) # è·å–å‚æ•°å­å­—å…¸
    try:
        # ç¡®ä¿åˆ—åæ­£ç¡® (å°å†™)
        df.columns = df.columns.str.lower()
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in df.columns]
            logging.error(f"Missing required columns for indicator calculation: {missing_cols}")
            return df # è¿”å›åŸå§‹dfæˆ–å¼•å‘é”™è¯¯

        logging.debug(f"Calculating indicators for DataFrame with shape: {df.shape}. Initial columns: {df.columns.tolist()}")

        # 1. EMA (è¶‹åŠ¿)
        ema_short = params.get('ema_short_period', 10)
        ema_long = params.get('ema_long_period', 30)
        df.ta.ema(length=ema_short, append=True, col_names=(f'EMA_{ema_short}',))
        df.ta.ema(length=ema_long, append=True, col_names=(f'EMA_{ema_long}',))
        logging.debug(f"EMA_{ema_short} and EMA_{ema_long} calculated.")

        # 2. ROC (åŠ¨é‡)
        roc_period = params.get('roc_period', 9)
        df.ta.roc(length=roc_period, append=True, col_names=(f'ROC_{roc_period}',))
        logging.debug(f"ROC_{roc_period} calculated.")

        # 3. RSI (è¶…ä¹°è¶…å–)
        rsi_period = params.get('rsi_period', 14)
        df.ta.rsi(length=rsi_period, append=True, col_names=(f'RSI_{rsi_period}',))
        logging.debug(f"RSI_{rsi_period} calculated.")

        # 4. Bollinger Bands (æ³¢åŠ¨æ€§/é€šé“)
        bb_period = params.get('bb_period', 20)
        bb_std = params.get('bb_std_dev', 2)
        # Note: ta.bbands appends multiple columns like BBL_20_2.0, BBM_20_2.0 etc.
        df.ta.bbands(length=bb_period, std=bb_std, append=True)
        logging.debug(f"Bollinger Bands (period={bb_period}, std={bb_std}) calculated.")

        # 5. MACD (åŠ¨é‡/è¶‹åŠ¿)
        macd_fast = params.get('macd_fast_period', 12)
        macd_slow = params.get('macd_slow_period', 26)
        macd_signal = params.get('macd_signal_period', 9)
        # Note: ta.macd appends MACD_12_26_9, MACDh_12_26_9, MACDs_12_26_9
        df.ta.macd(fast=macd_fast, slow=macd_slow, signal=macd_signal, append=True)
        logging.debug(f"MACD (fast={macd_fast}, slow={macd_slow}, signal={macd_signal}) calculated.")

        # 6. Volume MA (æˆäº¤é‡)
        vol_ma_period = params.get('volume_ma_period', 20)
        df[f'Vol_MA_{vol_ma_period}'] = df['volume'].rolling(window=vol_ma_period).mean()
        logging.debug(f"Volume MA_{vol_ma_period} calculated.")

        # --- æ–°å¢æŒ‡æ ‡ ---

        # 7. KDJ (éšæœºæŒ‡æ ‡)
        kdj_len = params.get('kdj_length', 9)
        kdj_sig = params.get('kdj_signal', 3)
        kdj_k = params.get('kdj_k_period', 3) # This k parameter is for internal smoothing, might not appear in column name
        # !!! FIX: Adjust original column names based on DEBUG logs !!!
        kdj_cols_orig = [f'K_{kdj_len}_{kdj_sig}', f'D_{kdj_len}_{kdj_sig}', f'J_{kdj_len}_{kdj_sig}'] 
        kdj_cols_new = ['KDJ_K', 'KDJ_D', 'KDJ_J']
        try:
            df.ta.kdj(length=kdj_len, signal=kdj_sig, k=kdj_k, append=True)
            logging.debug(f"Columns after KDJ calculation (before rename): {df.columns.tolist()}") 
            # é‡å‘½å KDJ åˆ— (using corrected original names)
            rename_dict = dict(zip(kdj_cols_orig, kdj_cols_new))
            df.rename(columns=rename_dict, inplace=True, errors='ignore')
            logging.debug(f"KDJ (length={kdj_len}, signal={kdj_sig}, k={kdj_k}) calculated. Attempted rename using {rename_dict}.")
            # Verify rename success, add NA if still missing
            for i, col_new in enumerate(kdj_cols_new):
                 if col_new not in df.columns:
                      logging.warning(f"Column '{col_new}' is missing after attempted KDJ rename from '{kdj_cols_orig[i]}'. Adding as NA.")
                      df[col_new] = pd.NA
        except Exception as e:
            logging.warning(f"KDJ calculation/rename failed: {e}. Adding NA columns {kdj_cols_new}.")
            for col in kdj_cols_new:
                if col not in df.columns: df[col] = pd.NA

        # 8. Ichimoku Cloud (ä¸€ç›®å‡è¡¡è¡¨)
        ichi_tenkan = params.get('ichimoku_tenkan', 9)
        ichi_kijun = params.get('ichimoku_kijun', 26)
        ichi_senkou_b_period = params.get('ichimoku_senkou_b', 52) # Config name for the period used in calculation
        ichi_cols_new = ['Ichi_Tenkan', 'Ichi_Kijun', 'Ichi_SenkouA', 'Ichi_SenkouB', 'Ichi_Chikou']
        try:
            ichi_df, _ = df.ta.ichimoku(tenkan=ichi_tenkan, kijun=ichi_kijun, senkou=ichi_senkou_b_period, include_chikou=True, append=False)
            if ichi_df is not None:
                logging.debug(f"Ichimoku DataFrame columns returned by ta.ichimoku: {ichi_df.columns.tolist()}")
            else:
                logging.warning("ta.ichimoku returned None for ichi_df")
                raise ValueError("Ichimoku calculation returned None.")
                
            if not ichi_df.empty:
                 # Build the rename map dynamically based on DEBUG logs
                 rename_map = {}
                 # Tenkan
                 tenkan_col_orig = f'ITS_{ichi_tenkan}'
                 if tenkan_col_orig in ichi_df.columns: rename_map[tenkan_col_orig] = 'Ichi_Tenkan'
                 # Kijun
                 kijun_col_orig = f'IKS_{ichi_kijun}'
                 if kijun_col_orig in ichi_df.columns: rename_map[kijun_col_orig] = 'Ichi_Kijun'
                 # !!! FIX: Senkou A name based on DEBUG logs !!!
                 senkou_a_col_orig = f'ISA_{ichi_tenkan}' # Was ISA_9 in logs
                 if senkou_a_col_orig in ichi_df.columns: rename_map[senkou_a_col_orig] = 'Ichi_SenkouA'
                 # !!! FIX: Senkou B name based on DEBUG logs !!!
                 senkou_b_col_orig = f'ISB_{ichi_kijun}' # Was ISB_26 in logs (uses kijun period)
                 if senkou_b_col_orig in ichi_df.columns: rename_map[senkou_b_col_orig] = 'Ichi_SenkouB'
                 # Chikou
                 chikou_col_orig = f'ICS_{ichi_kijun}'
                 if chikou_col_orig in ichi_df.columns: rename_map[chikou_col_orig] = 'Ichi_Chikou'
                 
                 logging.debug(f"Ichimoku rename map constructed: {rename_map}")
                 
                 if not rename_map:
                      logging.warning("Ichimoku rename map is empty. Cannot merge Ichimoku columns.")
                 else:
                      cols_to_select = list(rename_map.keys())
                      # Check if all expected new columns are in the map keys
                      missing_in_map = [ichi_cols_new[i] for i, orig_col in enumerate([tenkan_col_orig, kijun_col_orig, senkou_a_col_orig, senkou_b_col_orig, chikou_col_orig]) if orig_col not in rename_map]
                      if missing_in_map:
                          logging.warning(f"Could not find original columns in ichi_df for: {missing_in_map}. Corresponding columns will be missing.")
                      
                      ichi_selected = ichi_df[cols_to_select].rename(columns=rename_map)
                      df = pd.concat([df, ichi_selected], axis=1)
                      logging.debug(f"Ichimoku columns merged and renamed: {ichi_selected.columns.tolist()}")
                      # Add NA for any columns that failed to map/merge
                      for col_new in ichi_cols_new:
                          if col_new not in df.columns:
                              logging.warning(f"Column '{col_new}' is missing after Ichimoku merge. Adding as NA.")
                              df[col_new] = pd.NA
            else:
                 raise ValueError("Ichimoku calculation returned empty DataFrame.")
        except Exception as e:
            logging.warning(f"Ichimoku calculation/merge failed: {e}. Adding NA columns {ichi_cols_new}.")
            for col in ichi_cols_new:
                if col not in df.columns: df[col] = pd.NA

        # 9. ADX (Already confirmed working from logs, keeping rename)
        adx_len = params.get('adx_length', 14)
        adx_cols_orig = [f'ADX_{adx_len}', f'DMP_{adx_len}', f'DMN_{adx_len}']
        adx_cols_new = ['ADX', 'ADX_DIp', 'ADX_DIn']
        try:
            df.ta.adx(length=adx_len, append=True)
            logging.debug(f"Columns after ADX calculation (before rename): {df.columns.tolist()}")
            rename_dict = dict(zip(adx_cols_orig, adx_cols_new))
            df.rename(columns=rename_dict, inplace=True, errors='ignore')
            logging.debug(f"ADX (length={adx_len}) calculated. Attempted rename using {rename_dict}.")
            # Verify rename success, add NA if still missing
            for i, col_new in enumerate(adx_cols_new):
                 if col_new not in df.columns:
                      logging.warning(f"Column '{col_new}' is missing after attempted ADX rename from '{adx_cols_orig[i]}'. Adding as NA.")
                      df[col_new] = pd.NA
        except Exception as e:
            logging.warning(f"ADX calculation/rename failed: {e}. Adding NA columns {adx_cols_new}.")
            for col in adx_cols_new:
                if col not in df.columns: df[col] = pd.NA

        # --- End of New Indicators ---

        logging.info(f"All indicators calculation attempt finished. DataFrame final shape: {df.shape}")
        logging.debug(f"Final columns before returning from _calculate_indicators: {df.columns.tolist()}")

    except Exception as e:
        logging.error(f"Critical error during indicator calculation: {e}", exc_info=True)
        # Depending on policy, return df as is, or return None, or raise error
        # Returning df might lead to errors downstream if columns are missing
        # Adding expected columns as NA might be safer if downstream needs them
        expected_cols = [f'EMA_{ema_short}', f'EMA_{ema_long}', f'ROC_{roc_period}', f'RSI_{rsi_period}',
                         f'BBL_{bb_period}_{bb_std}.0', f'BBM_{bb_period}_{bb_std}.0', f'BBU_{bb_period}_{bb_std}.0', # Adjust BB names if needed
                         f'MACD_{macd_fast}_{macd_slow}_{macd_signal}', f'MACDh_{macd_fast}_{macd_slow}_{macd_signal}', f'MACDs_{macd_fast}_{macd_slow}_{macd_signal}',
                         f'Vol_MA_{vol_ma_period}'] + kdj_cols_new + ichi_cols_new + adx_cols_new
        for col in expected_cols:
             if col not in df.columns: df[col] = pd.NA
        logging.warning("Returning DataFrame potentially with NAs due to calculation error.")

    return df

# --- å•å‘¨æœŸåˆ†æå‡½æ•° ---
# é‡å‘½åï¼šåˆ†æå¾®è§‚è¶‹åŠ¿ -> åˆ†æå•å‘¨æœŸè¶‹åŠ¿
def åˆ†æå•å‘¨æœŸè¶‹åŠ¿(df_with_indicators: pd.DataFrame, config: dict, interval: str = "") -> Dict[str, Union[str, float, Dict[str, str], None]]:
    """
    åˆ†æå•ä¸ªæ—¶é—´å‘¨æœŸçš„ã€å·²åŒ…å«æŒ‡æ ‡çš„DataFrameï¼Œæå–æœ€æ–°ä¿¡å·ä¿¡æ¯ã€‚

    Args:
        df_with_indicators (pd.DataFrame): å·²è®¡ç®—æŒ‡æ ‡çš„ K çº¿æ•°æ®ã€‚
        config (dict): åŒ…å«è®¡ç®—å’Œè§£è¯»æ‰€éœ€å‚æ•°çš„é…ç½®å­—å…¸ã€‚
        interval (str, optional): å½“å‰åˆ†æçš„æ—¶é—´å‘¨æœŸ (ç”¨äºæ—¥å¿—æˆ–é”™è¯¯ä¿¡æ¯). Defaults to "".

    Returns:
        Dict: åŒ…å« 'interval', 'combined_signal', 'score', 'details' çš„å­—å…¸ï¼Œ
              æˆ–è€…åœ¨é”™è¯¯æ—¶åŒ…å« 'error' é”®ã€‚
    """
    interval_prefix = f"[{interval}] " if interval else ""
    result_base = {"interval": interval, "combined_signal": None, "score": None, "details": None} 
    
    if df_with_indicators is None or df_with_indicators.empty:
        logger.warning(f"{interval_prefix}è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œæ— æ³•åˆ†æã€‚")
        result_base["error"] = "è¾“å…¥æ•°æ®ä¸ºç©º"
        return result_base

    if len(df_with_indicators) < 2:
        logger.warning(f"{interval_prefix}æ•°æ®ä¸è¶³ (<2 è¡Œ)ï¼Œæ— æ³•è®¡ç®—äº¤å‰ä¿¡å·ã€‚")
        # Decide how to handle this - maybe still try to interpret?
        # For now, let's return an error state for consistency in MTF analysis
        result_base["error"] = "æ•°æ®è¡Œæ•°ä¸è¶³ (<2)"
        # If you want to proceed with single-row analysis, adjust here
        # last_row = df_with_indicators.iloc[-1]
        # prev_row = None 
        return result_base 
    else:
        last_row = df_with_indicators.iloc[-1]
        prev_row = df_with_indicators.iloc[-2]

    try:
        # --- åŠ¨æ€ç¡®å®šåˆ—å --- 
        params = config.get('PARAMS', {})
        ema_short_period = params.get('ema_short_period', 10)
        ema_long_period = params.get('ema_long_period', 30)
        roc_period = params.get('roc_period', 9)
        rsi_period = params.get('rsi_period', 14)
        bb_period = params.get('bb_period', 20)
        bb_std_dev = params.get('bb_std_dev', 2.0)
        macd_fast = params.get('macd_fast_period', 12)
        macd_slow = params.get('macd_slow_period', 26)
        macd_signal = params.get('macd_signal_period', 9)
        volume_ma_period = params.get('volume_ma_period', 20)

        ema_short_col = f'EMA_{ema_short_period}'
        ema_long_col = f'EMA_{ema_long_period}'
        roc_col = f'ROC_{roc_period}'
        rsi_col = f'RSI_{rsi_period}'
        # pandas_ta bbands åˆ—åå¯èƒ½åŒ…å«å°æ•°ç‚¹ (e.g., BBU_20_2.0)
        bb_std_str = f"{bb_std_dev:.1f}" # Format std dev for column name
        bb_upper_col = f'BBU_{bb_period}_{bb_std_str}'
        bb_lower_col = f'BBL_{bb_period}_{bb_std_str}'
        bb_mid_col = f'BBM_{bb_period}_{bb_std_str}' # ä¸­è½¨åˆ—å
        # MACD Histogram åˆ—åçº¦å®šä¸º 'MACDh_...' (æ³¨æ„å°å†™ h)
        macd_hist_col = f'MACDh_{macd_fast}_{macd_slow}_{macd_signal}'
        volume_col = 'volume' # Assuming original volume column exists
        volume_ma_col = f'Vol_MA_{volume_ma_period}'

        # æ–°æŒ‡æ ‡ (ä½¿ç”¨ _calculate_indicators ä¸­é‡å‘½ååçš„åˆ—å)
        kdj_k_col = 'KDJ_K'
        kdj_d_col = 'KDJ_D'
        kdj_j_col = 'KDJ_J'
        ichi_tenkan_col = 'Ichi_Tenkan'
        ichi_kijun_col = 'Ichi_Kijun'
        ichi_senkou_a_col = 'Ichi_SenkouA'
        ichi_senkou_b_col = 'Ichi_SenkouB'
        ichi_chikou_col = 'Ichi_Chikou'
        adx_col = 'ADX'
        adx_dip_col = 'ADX_DIp' # DI+
        adx_din_col = 'ADX_DIn' # DI-
        
        # --- æ£€æŸ¥åˆ— --- 
        required_cols_for_interpret = [
            ema_short_col, ema_long_col, roc_col, rsi_col, 
            bb_upper_col, bb_lower_col, bb_mid_col, 
            macd_hist_col, volume_col, volume_ma_col,
            kdj_k_col, kdj_d_col, kdj_j_col,
            ichi_tenkan_col, ichi_kijun_col, ichi_senkou_a_col, ichi_senkou_b_col, ichi_chikou_col,
            adx_col, adx_dip_col, adx_din_col
        ]
        missing_cols = [col for col in required_cols_for_interpret if col not in df_with_indicators.columns]
        if missing_cols:
            logger.error(f"{interval_prefix}æ— æ³•è§£è¯»æŒ‡æ ‡ï¼Œç¼ºå°‘åˆ—: {missing_cols}")
            result_base["error"] = f"ç¼ºå°‘æŒ‡æ ‡åˆ—: {', '.join(missing_cols)}"
            return result_base

        # --- è°ƒç”¨è§£è¯»å‡½æ•° --- 
        interpretation_result = _interpret_indicators(
            last_row, prev_row, config,
            ema_short_col, ema_long_col, roc_col, rsi_col,
            bb_upper_col, bb_lower_col, bb_mid_col,
            macd_hist_col, volume_col, volume_ma_col,
            kdj_k_col, kdj_d_col, kdj_j_col,
            ichi_tenkan_col, ichi_kijun_col, ichi_senkou_a_col, ichi_senkou_b_col, ichi_chikou_col,
            adx_col, adx_dip_col, adx_din_col
        )
        
        # --- ç»„åˆæœ€ç»ˆç»“æœå­—å…¸ --- 
        result_base.update(interpretation_result) # Merge results from _interpret_indicators
        return result_base

    except Exception as e:
        logger.error(f"{interval_prefix}Error in åˆ†æå•å‘¨æœŸè¶‹åŠ¿: {e}", exc_info=True)
        result_base["error"] = "åˆ†æè¿‡ç¨‹ä¸­æ–­"
        return result_base

# --- å¤šå‘¨æœŸåˆ†æåè°ƒå‡½æ•° (æ–°å¢) ---
def æ‰§è¡Œå¤šå‘¨æœŸåˆ†æ(symbol: str, market_type: str, intervals: List[str], config: dict, kline_limit_base: int = 100) -> Dict[str, Dict[str, Union[str, float, Dict[str, str], None]]]:
    """
    æ‰§è¡Œå¤šå‘¨æœŸå¾®è§‚è¶‹åŠ¿åˆ†æã€‚

    Args:
        symbol (str): äº¤æ˜“å¯¹ã€‚
        market_type (str): å¸‚åœºç±»å‹ ('spot' or 'futures')ã€‚
        intervals (list): éœ€è¦åˆ†æçš„æ—¶é—´å‘¨æœŸåˆ—è¡¨ (e.g., ['1m', '5m', '15m', '1h'])ã€‚
        config (dict): æŒ‡æ ‡è®¡ç®—å’Œè§£è¯»çš„é…ç½®ã€‚
        kline_limit_base (int): è·å–Kçº¿çš„åŸºç¡€æ•°é‡ï¼Œä¼šæ ¹æ®æŒ‡æ ‡å‘¨æœŸé€‚å½“è°ƒæ•´ã€‚

    Returns:
        Dict: é”®æ˜¯æ—¶é—´å‘¨æœŸ, å€¼æ˜¯åŒ…å«è¯¥å‘¨æœŸåˆ†æç»“æœçš„å­—å…¸ (æ¥è‡ª åˆ†æå•å‘¨æœŸè¶‹åŠ¿)ã€‚
    """
    results: Dict[str, Dict[str, Union[str, float, Dict[str, str], None]]] = {} # Type hint added
    if æ•°æ®è·å–æ¨¡å— is None:
        logger.error("æ•°æ®è·å–æ¨¡å—æœªåŠ è½½ï¼Œæ— æ³•æ‰§è¡Œå¤šå‘¨æœŸåˆ†æã€‚")
        return {"error": "æ•°æ®è·å–æ¨¡å—ä¸å¯ç”¨"}

    # ç¡®å®šè®¡ç®—æŒ‡æ ‡æ‰€éœ€çš„æœ€å°‘ K çº¿æ•°é‡ (åŸºäºé…ç½®ä¸­çš„æœ€é•¿å‘¨æœŸ)
    # (è¿™ä¸ªé€»è¾‘å¯ä»¥æ›´ç²¾ç¡®ï¼Œä½†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å…ˆç”¨ä¸€ä¸ªåŸºæ•°ï¼Œæˆ–å–é…ç½®ä¸­çš„æœ€å¤§å€¼)
    max_period = max(
        config.get('ema_long_period', 30),
        config.get('bb_period', 20),
        config.get('macd_slow_period', 26) + config.get('macd_signal_period', 9),
        config.get('volume_ma_period', 20),
        config.get('kdj_length', 9) + config.get('kdj_signal', 3),
        config.get('ichimoku_senkou_b', 52) + config.get('ichimoku_kijun', 26), # Ichimoku éœ€è¦è€ƒè™‘ä½ç§»
        config.get('adx_length', 14) * 2 # ADX è®¡ç®—æ¯”è¾ƒå¤æ‚ï¼Œç»™äº›ä½™é‡
    )
    required_limit = max(kline_limit_base, max_period + 5) # åŠ ä¸€ç‚¹ buffer
    logger.info(f"å¤šå‘¨æœŸåˆ†æå°†ä¸ºæ¯ä¸ªå‘¨æœŸè¯·æ±‚ {required_limit} æ¡ K çº¿æ•°æ®ã€‚")

    for interval in intervals:
        logger.info(f"--- å¼€å§‹åˆ†æå‘¨æœŸ: {interval} ---")
        # Store analysis result dictionary directly
        results[interval] = {"interval": interval, "combined_signal": "åˆ†æä¸­...", "score": None, "details": None}
        try:
            # 1. è·å– K çº¿æ•°æ®
            logger.debug(f"[{interval}] è·å– {symbol} {market_type} {interval} Kçº¿æ•°æ®, limit={required_limit}...")
            kline_data = æ•°æ®è·å–æ¨¡å—.è·å–Kçº¿æ•°æ®(
                symbol=symbol,
                interval=interval,
                limit=required_limit,
                market_type=market_type
            )

            if kline_data is None or kline_data.empty or len(kline_data) < 2: # éœ€è¦è‡³å°‘2æ¡æ‰èƒ½åˆ†æ
                logger.warning(f"[{interval}] æœªèƒ½è·å–åˆ°è¶³å¤Ÿçš„ K çº¿æ•°æ® (è·å–åˆ° {len(kline_data) if kline_data is not None else 0} æ¡)ã€‚")
                results[interval] = {"interval": interval, "error": "Kçº¿æ•°æ®ä¸è¶³"}
                continue # ç»§ç»­ä¸‹ä¸€ä¸ªå‘¨æœŸ
            
            logger.debug(f"[{interval}] æˆåŠŸè·å– {len(kline_data)} æ¡Kçº¿æ•°æ®.")
            
            # 2. è®¡ç®—æŒ‡æ ‡
            data_with_indicators = _calculate_indicators(kline_data.copy(), config)

            if data_with_indicators is None or data_with_indicators.empty:
                logger.error(f"[{interval}] æŒ‡æ ‡è®¡ç®—å¤±è´¥æˆ–è¿”å›ç©ºçš„ DataFrame.")
                results[interval] = {"interval": interval, "error": "æŒ‡æ ‡è®¡ç®—å¤±è´¥"}
                continue
                
            logger.debug(f"[{interval}] æŒ‡æ ‡è®¡ç®—å®Œæˆ.")
                
            # 3. åˆ†æå•å‘¨æœŸè¶‹åŠ¿ (è°ƒç”¨è¿”å›å­—å…¸çš„å‡½æ•°) --- 
            single_interval_result = åˆ†æå•å‘¨æœŸè¶‹åŠ¿(data_with_indicators, config, interval=interval)
            results[interval] = single_interval_result # Store the entire result dictionary
            # Log the combined signal and score for info
            log_signal = single_interval_result.get('combined_signal', 'N/A')
            log_score = single_interval_result.get('score')
            log_error = single_interval_result.get('error')
            if log_error:
                 logger.info(f"[{interval}] åˆ†æå®Œæˆ (æœ‰é”™è¯¯): {log_error}")
            else:
                 logger.info(f"[{interval}] åˆ†æå®Œæˆ: ç»„åˆ:{log_signal} (è¯„åˆ†:{log_score})")

        except Exception as e:
            logger.error(f"[{interval}] å¤„ç†å‘¨æœŸæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
            results[interval] = {"interval": interval, "error": "å‘¨æœŸå¤„ç†å¼‚å¸¸"}
            
    logger.info("--- å¤šå‘¨æœŸåˆ†æå…¨éƒ¨å®Œæˆ ---")
    return results

# --- å¤šå‘¨æœŸä¿¡å·æ•´åˆå‡½æ•° (ä¼˜åŒ–) ---
def æ•´åˆå¤šå‘¨æœŸä¿¡å·(mtf_results: Dict[str, Dict[str, Union[str, float, Dict[str, str], None]]],
                   config: dict) -> Dict[str, Union[str, List[str], float, None]]: # è¿”å›ç±»å‹å¯èƒ½å¢åŠ  score
    """
    æ ¹æ®é…ç½®è§„åˆ™æ•´åˆå¤šå‘¨æœŸåˆ†æç»“æœï¼Œç”Ÿæˆç»“æ„åŒ–åˆ¤æ–­ã€‚
    æ–°è§„åˆ™ï¼šåŸºäºé…ç½®çš„å‘¨æœŸåˆ—è¡¨å’Œæƒé‡è¿›è¡ŒåŠ æƒè¯„åˆ†ï¼Œå¹¶æ£€æµ‹å†²çªã€‚

    Args:
        mtf_results (Dict): æ‰§è¡Œå¤šå‘¨æœŸåˆ†æè¿”å›çš„ç»“æœå­—å…¸ã€‚
        config (dict): åŒ…å« 'INTEGRATION' é…ç½®çš„å­—å…¸ã€‚

    Returns:
        Dict: åŒ…å«æ•´åˆç»“æœçš„å­—å…¸, e.g., 
              {'type': 'StrongConfirmation', 'direction': 'Bullish', 'score': 2.5, 
               'periods_involved': ['1m', '5m', '15m', '1h', '4h'], 'message': 'å¼ºçƒˆçœ‹æ¶¨ç¡®è®¤...'}
              or {'type': 'Conflicting', 'direction': 'Neutral', ...}
    """
    # !! ä¿®æ”¹ï¼šç›´æ¥ä»å…¨å±€é…ç½®è¯»å–ï¼Œä¸å†ä¾èµ–ä¼ å…¥çš„ config['INTEGRATION'] !!
    try:
        intervals_to_integrate = getattr(é…ç½®, 'MOMENTUM_INTEGRATION_TIMEFRAMES', ['1m', '5m', '15m', '1h', '4h'])
        interval_weights = getattr(é…ç½®, 'MOMENTUM_INTEGRATION_WEIGHTS', {})
        conflict_diff_threshold = getattr(é…ç½®, 'MOMENTUM_CONFLICT_SCORE_DIFF_THRESHOLD', 5.0)
        # åŠ æƒé˜ˆå€¼å¯ä»¥ä¿ç•™åœ¨å‡½æ•°å†…éƒ¨æˆ–ä¹Ÿç§»åˆ°é…ç½®
        weighted_bull_threshold = 1.5
        weighted_bear_threshold = -1.5
        conflict_ratio_threshold = 0.4 # é»˜è®¤40%
    except AttributeError as e:
        logger.error(f"æ— æ³•ä» é…ç½®.py è¯»å–æ•´åˆé…ç½®: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
        intervals_to_integrate = ['1m', '5m', '15m', '1h', '4h']
        interval_weights = {}
        conflict_diff_threshold = 5.0
        weighted_bull_threshold = 1.5
        weighted_bear_threshold = -1.5
        conflict_ratio_threshold = 0.4
        
    # å•é¡¹è¯„åˆ†çš„é˜ˆå€¼ (ä»ä¼ å…¥çš„ config['SCORING'] è·å–)
    scoring_thresholds = config.get('SCORING', {}).get('THRESHOLDS', {})
    single_bull_threshold = scoring_thresholds.get('bullish', 1.0) # é»˜è®¤ 1.0
    single_bear_threshold = scoring_thresholds.get('bearish', -1.0) # é»˜è®¤ -1.0

    logger.info(f"å¼€å§‹æ•´åˆä¿¡å·. æ•´åˆå‘¨æœŸ: {intervals_to_integrate}, æƒé‡: {interval_weights}, å†²çªé˜ˆå€¼: {conflict_diff_threshold}")

    # --- æå–ç›¸å…³å‘¨æœŸçš„æœ‰æ•ˆè¯„åˆ†å’Œæƒé‡ --- 
    valid_scores = {}
    total_weighted_score = 0.0
    total_weight = 0.0
    periods_involved = []
    
    for interval in intervals_to_integrate:
        result = mtf_results.get(interval)
        weight = interval_weights.get(interval)
        
        if result and not result.get("error") and result.get("score") is not None and weight is not None:
            score = result["score"]
            valid_scores[interval] = score
            total_weighted_score += score * weight
            total_weight += weight
            periods_involved.append(interval)
        elif weight is None:
             logger.warning(f"è·³è¿‡å‘¨æœŸ {interval}ï¼šæœªåœ¨é…ç½®ä¸­æ‰¾åˆ°æƒé‡ã€‚")
        # else: å¿½ç•¥é”™è¯¯æˆ–æ— åˆ†æ•°çš„å‘¨æœŸ

    # --- æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆå‘¨æœŸæ•°æ® --- 
    if not valid_scores or total_weight == 0:
        msg = f"æ— æ³•æ•´åˆï¼šç¼ºå°‘è¶³å¤Ÿçš„å¯ç”¨äºæ•´åˆçš„å‘¨æœŸæ•°æ®æˆ–æœ‰æ•ˆæƒé‡ã€‚å‚ä¸å‘¨æœŸ: {periods_involved}"
        logger.warning(msg)
        return {"type": "Error", "direction": "Neutral", "score": None, "periods_involved": periods_involved, "message": msg}
        
    # --- è®¡ç®—åŠ æƒå¹³å‡åˆ† --- 
    average_weighted_score = total_weighted_score / total_weight
    logger.info(f"è®¡ç®—å®Œæˆ: æ€»åŠ æƒåˆ†={total_weighted_score:.2f}, æ€»æƒé‡={total_weight:.2f}, å¹³å‡åŠ æƒåˆ†={average_weighted_score:.2f}")

    # --- åˆæ­¥åˆ¤æ–­æ–¹å‘ --- 
    preliminary_direction = "Neutral"
    if average_weighted_score >= weighted_bull_threshold:
        preliminary_direction = "Bullish"
    elif average_weighted_score <= weighted_bear_threshold:
        preliminary_direction = "Bearish"
        
    # --- å†²çªæ£€æµ‹ --- 
    is_conflicting = False
    conflict_reasons = []
    num_bullish_periods = 0
    num_bearish_periods = 0
    scores_list = list(valid_scores.values())
    max_score = max(scores_list)
    min_score = min(scores_list)
    
    for score in scores_list:
        if score >= single_bull_threshold: # ä½¿ç”¨å•é¡¹è¯„åˆ†é˜ˆå€¼ç»Ÿè®¡
            num_bullish_periods += 1
        elif score <= single_bear_threshold:
            num_bearish_periods += 1
            
    num_total_periods = len(scores_list)
    
    # 1. æ£€æŸ¥è¯„åˆ†å·®å¼‚
    score_diff = max_score - min_score
    if score_diff >= conflict_diff_threshold:
        is_conflicting = True
        conflict_reasons.append(f"è¯„åˆ†å·®å¼‚è¿‡å¤§({score_diff:.1f} >= {conflict_diff_threshold:.1f})ï¼ŒMax={max_score:.1f}, Min={min_score:.1f}")
        logger.debug("å†²çªæ£€æµ‹ï¼šè¯„åˆ†å·®å¼‚è¿‡å¤§")
        
    # 2. æ£€æŸ¥å¤šç©ºå‘¨æœŸæ¯”ä¾‹
    if num_total_periods > 1: # è‡³å°‘éœ€è¦ä¸¤ä¸ªå‘¨æœŸæ‰èƒ½æ¯”è¾ƒæ¯”ä¾‹
        if num_bullish_periods > 0 and num_bearish_periods > 0: # åŒæ—¶å­˜åœ¨å¤šç©ºä¿¡å·
            # è®¡ç®—å°‘æ•°æ´¾å æ¯”
            min_periods = min(num_bullish_periods, num_bearish_periods)
            minority_ratio = min_periods / num_total_periods
            if minority_ratio >= conflict_ratio_threshold:
                 is_conflicting = True
                 conflict_reasons.append(f"å¤šç©ºå‘¨æœŸæ¯”ä¾‹å†²çª(å°‘æ•°æ´¾å æ¯” {minority_ratio:.2f} >= {conflict_ratio_threshold:.2f}ï¼Œå¤š:{num_bullish_periods},ç©º:{num_bearish_periods})")
                 logger.debug("å†²çªæ£€æµ‹ï¼šå¤šç©ºå‘¨æœŸæ¯”ä¾‹å†²çª")
        elif num_bullish_periods == 0 and num_bearish_periods == 0: # å…¨æ˜¯ä¸­æ€§æˆ–è¯„åˆ†åœ¨é˜ˆå€¼ä¹‹é—´
             pass # ä¸æ˜¯å…¸å‹çš„å†²çªï¼Œä½†ä¹Ÿä¸æ˜¯å¼ºä¿¡å·

    # --- ç¡®å®šæœ€ç»ˆç±»å‹å’Œæ¶ˆæ¯ --- 
    final_type = "Neutral"
    final_direction = preliminary_direction
    message_parts = [f"æ•´åˆä¿¡å·(å‡åˆ†:{average_weighted_score:.2f})"]

    if is_conflicting:
        final_type = "Conflicting"
        final_direction = "Neutral" # å†²çªæ—¶æ–¹å‘å€¾å‘äºä¸­æ€§
        message_parts.append("ä¿¡å·å†²çª:")
        message_parts.extend([f"- {r}" for r in conflict_reasons])
        logger.info(f"æ•´åˆç»“æœ: ä¿¡å·å†²çª")
    else:
        # éå†²çªæƒ…å†µï¼Œæ ¹æ®å¹³å‡åˆ†å¼ºåº¦åˆ¤æ–­
        abs_score = abs(average_weighted_score)
        # ä½¿ç”¨åŠ æƒé˜ˆå€¼çš„å€æ•°æ¥åˆ¤æ–­å¼ºåº¦ (å¯ä»¥è°ƒæ•´è¿™ä¸ªé€»è¾‘)
        if preliminary_direction == "Bullish":
            if abs_score >= weighted_bull_threshold * 1.5: # æ¯”å¦‚è¶…è¿‡é˜ˆå€¼çš„1.5å€ç®—å¼ºç¡®è®¤
                final_type = "StrongConfirmation"
                message_parts.append("å¼ºçƒˆçœ‹æ¶¨ç¡®è®¤")
            else:
                final_type = "WeakConfirmation"
                message_parts.append("çœ‹æ¶¨ä¿¡å·(è¾ƒå¼±)")
            logger.info(f"æ•´åˆç»“æœ: {final_type} {preliminary_direction}")
        elif preliminary_direction == "Bearish":
            if abs_score >= abs(weighted_bear_threshold) * 1.5: # æ¯”å¦‚è¶…è¿‡é˜ˆå€¼çš„1.5å€ç®—å¼ºç¡®è®¤
                final_type = "StrongConfirmation"
                message_parts.append("å¼ºçƒˆçœ‹è·Œç¡®è®¤")
            else:
                final_type = "WeakConfirmation"
                message_parts.append("çœ‹è·Œä¿¡å·(è¾ƒå¼±)")
            logger.info(f"æ•´åˆç»“æœ: {final_type} {preliminary_direction}")
        else: # Neutral
             final_type = "Neutral"
             message_parts.append("ä¿¡å·ä¸­æ€§æˆ–å¼ºåº¦ä¸è¶³")
             logger.info(f"æ•´åˆç»“æœ: ä¸­æ€§")

    result_dict = {
        "type": final_type, 
        "direction": final_direction, 
        "score": round(average_weighted_score, 2), # ä¿ç•™ä¸¤ä½å°æ•°
        "periods_involved": periods_involved, 
        "message": "\n".join(message_parts)
    }
    
    # --- æ·»åŠ ä¸­æ–‡æ˜ å°„ --- 
    type_cn_map = {
        'StrongConfirmation': 'å¼ºåŠ›ç¡®è®¤',
        'WeakConfirmation': 'å¼±ç¡®è®¤',
        'Conflicting': 'ä¿¡å·å†²çª',
        'Neutral': 'ä¸­æ€§',
        'Error': 'é”™è¯¯',
        'Incomplete Data': 'æ•°æ®ä¸è¶³'
    }
    direction_cn_map = {
        'Bullish': 'çœ‹æ¶¨',
        'Bearish': 'çœ‹è·Œ',
        'Neutral': 'ä¸­æ€§'
    }
    
    result_dict['type'] = type_cn_map.get(final_type, final_type) # æ›¿æ¢ä¸ºä¸­æ–‡ï¼Œæ‰¾ä¸åˆ°åˆ™ä¿ç•™åŸæ–‡
    result_dict['direction'] = direction_cn_map.get(final_direction, final_direction) # æ›¿æ¢ä¸ºä¸­æ–‡
    
    return result_dict

# --- ä¸»é€»è¾‘å’Œæµ‹è¯• ---
if __name__ == '__main__':
    test_symbol = 'BTCUSDT'
    test_market_type = 'futures'
    # !!! æ›´æ–°ï¼šä»é…ç½®è¯»å–è¦åˆ†æçš„æ—¶é—´å‘¨æœŸåˆ—è¡¨ !!!
    try:
        intervals_to_analyze = getattr(é…ç½®, 'MOMENTUM_TIMEFRAMES', ['1m', '5m', '15m', '1h', '4h', '1d', '1w'])
        # ä»é…ç½®è¯»å–æµ‹è¯•å¾ªç¯å‚æ•°
        num_iterations = getattr(é…ç½®, 'TEST_LOOP_ITERATIONS', 2)
        interval_seconds = getattr(é…ç½®, 'TEST_LOOP_INTERVAL', 10)
    except AttributeError as e:
        logger.error(f"æ— æ³•ä» é…ç½®.py è¯»å–åˆ†æå‘¨æœŸæˆ–æµ‹è¯•å¾ªç¯å‚æ•°: {e}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼ã€‚")
        intervals_to_analyze = ['1m', '5m', '15m', '1h', '4h', '1d', '1w']
        num_iterations = 2
        interval_seconds = 10

    logger.info(f"--- æµ‹è¯• å¾®è§‚è¶‹åŠ¿åŠ¨é‡æ¨¡å— (å¤šå‘¨æœŸ) ({test_symbol}, {test_market_type.upper()}) ---")
    logger.info(f"è®¡åˆ’åˆ†æå‘¨æœŸ: {intervals_to_analyze}")
    logger.info(f"æµ‹è¯•å¾ªç¯æ¬¡æ•°: {num_iterations}, é—´éš”: {interval_seconds}ç§’") # æ·»åŠ æ—¥å¿—

    # --- è·å–é…ç½® --- 
    # ä» é…ç½® æ¨¡å—å¯¼å…¥é›†ä¸­ç®¡ç†çš„é…ç½®
    # (å·²åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥: from é…ç½® import MICRO_TREND_CONFIG)
    if é…ç½® is None or not hasattr(é…ç½®, 'MICRO_TREND_CONFIG'): # æ£€æŸ¥é…ç½®æ¨¡å—å’Œå˜é‡æ˜¯å¦å­˜åœ¨
         logger.error("é…ç½®.py æœªåŠ è½½æˆ–ç¼ºå°‘ MICRO_TREND_CONFIGï¼Œæ— æ³•æ‰§è¡Œåˆ†æã€‚")
         micro_trend_config_to_use = {} # ä½¿ç”¨ç©ºå­—å…¸ä»¥é˜²åç»­ä»£ç å‡ºé”™
    else:
        micro_trend_config_to_use = é…ç½®.MICRO_TREND_CONFIG
        logger.debug(f"ä½¿ç”¨å¯¼å…¥çš„é…ç½®: {micro_trend_config_to_use}")

    # --- æ‰§è¡Œåˆ†æ --- 
    # è°ƒç”¨å¤šå‘¨æœŸåˆ†æå‡½æ•°ï¼Œä¼ å…¥è·å–åˆ°çš„é…ç½®å­—å…¸
    mtf_results = æ‰§è¡Œå¤šå‘¨æœŸåˆ†æ(
        symbol=test_symbol,
        market_type=test_market_type,
        intervals=intervals_to_analyze,
        config=micro_trend_config_to_use, # ä½¿ç”¨ä»é…ç½®åŠ è½½çš„å­—å…¸
        kline_limit_base=100 
    )

    # --- æ‰“å°åŸå§‹å¤šå‘¨æœŸç»“æœ --- 
    print("\n--- Multi-Timeframe Analysis Results (Raw) ---")
    if isinstance(mtf_results, dict):
        max_interval_len = max(len(interval) for interval in mtf_results.keys()) if mtf_results else 3
        for interval in intervals_to_analyze:
            result = mtf_results.get(interval)
            if result and not result.get('error'):
                # æ ¼å¼åŒ–è¾“å‡ºï¼ŒåŒ…å«ä¿¡å·å’Œè¯„åˆ†
                signal_str = result.get('combined_signal', 'N/A')
                score_val = result.get('score')
                score_str = f"(è¯„åˆ†:{score_val:.1f})" if score_val is not None else ""
                print(f"{interval:>{max_interval_len}s}: ç»„åˆ:{signal_str} {score_str}")
            elif result and result.get('error'):
                 print(f"{interval:>{max_interval_len}s}: é”™è¯¯: {result.get('error')}")
            else:
                print(f"{interval:>{max_interval_len}s}: æœªåˆ†ææˆ–å‡ºé”™")
    else:
        print("å¤šå‘¨æœŸåˆ†ææœªèƒ½è¿”å›æœ‰æ•ˆç»“æœã€‚")

    # --- æ‰§è¡Œå¹¶æ‰“å°æ•´åˆä¿¡å· --- 
    if isinstance(mtf_results, dict) and intervals_to_analyze:
        # è°ƒç”¨æ•´åˆå‡½æ•° (ç°åœ¨ç›´æ¥ä» é…ç½® è¯»å–æ•´åˆå‚æ•°ï¼Œä½†ä»éœ€ä¼ å…¥æŒ‡æ ‡è®¡ç®—ç›¸å…³çš„ config)
        integration_result = æ•´åˆå¤šå‘¨æœŸä¿¡å·(mtf_results, config=micro_trend_config_to_use)
        print("\n--- Integrated Multi-Timeframe Signal --- ")
        # æ‰“å°ç»“æ„åŒ–ç»“æœ
        if isinstance(integration_result, dict):
            print(f"  ç±»å‹: {integration_result.get('type', 'N/A')}")
            print(f"  æ–¹å‘: {integration_result.get('direction', 'N/A')}")
            print(f"  æ¶‰åŠå‘¨æœŸ: {integration_result.get('periods_involved', [])}")
            print(f"  æ¶ˆæ¯: {integration_result.get('message', 'N/A')}")
        else:
             print(f"æ•´åˆä¿¡å·è¿”å›æ ¼å¼é”™è¯¯: {integration_result}")
    else:
        logger.warning("æ— æ³•æ‰§è¡Œä¿¡å·æ•´åˆï¼Œå› ä¸ºå¤šå‘¨æœŸåˆ†æç»“æœæ— æ•ˆæˆ–æœªæ‰§è¡Œã€‚")
